{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6086485",
   "metadata": {},
   "source": [
    "## Scrapping Data\n",
    "Scrapping Data from https://www.nhp.gov.in/disease-a-z/ of various diseases and their symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc557117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from googlesearch import search\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Fetch disease list from 'www.nhp.gov.in'\n",
    "small_alpha = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "diseases=[]\n",
    "for c in small_alpha:\n",
    "    URL = 'https://www.nhp.gov.in/disease-a-z/'+c\n",
    "    time.sleep(1)\n",
    "    page = requests.get(URL,verify=False)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html5lib')\n",
    "    all_diseases = soup.find('div', class_='all-disease')\n",
    "\n",
    "    for element in all_diseases.find_all('li'):\n",
    "        diseases.append(element.get_text().strip())\n",
    "\n",
    "with open('list_diseaseNames.pkl', 'rb') as handle:\n",
    "    diseases2 = pickle.load(handle)\n",
    "\n",
    "#print(len(diseases2))\n",
    "#print(len(diseases))\n",
    "#print(len(set(diseases).intersection(set(diseases2))))\n",
    "\n",
    "a=set(diseases)\n",
    "b=set(diseases2)\n",
    "c=list(a.union(b))\n",
    "c.sort()\n",
    "\n",
    "#print(c)\n",
    "\n",
    "# Search diseases on google, open wikipedia page and fetch symptom from infobox\n",
    "\n",
    "dis_symp={}\n",
    "# dis1=['anthrax']\n",
    "for dis in c:\n",
    "  query = dis+' wikipedia'\n",
    "   # search \"disease wilipedia\" on google \n",
    "  for sr in search(query,tld=\"co.in\",stop=10,pause=0.5): \n",
    "       # open wikipedia link\n",
    "    match=re.search(r'wikipedia',sr)\n",
    "    filled = 0\n",
    "    if match:\n",
    "      wiki = requests.get(sr,verify=False)\n",
    "      soup = BeautifulSoup(wiki.content, 'html5lib')\n",
    "       # Fetch HTML code for 'infobox'\n",
    "      info_table = soup.find(\"table\", {\"class\":\"infobox\"})\n",
    "      if info_table is not None:\n",
    "          # Preprocess contents of infobox\n",
    "        for row in info_table.find_all(\"tr\"):\n",
    "          data=row.find(\"th\",{\"scope\":\"row\"})\n",
    "          if data is not None:\n",
    "            data=data.get_text()\n",
    "            if data==\"Symptoms\":\n",
    "              symptom=str(row.find(\"td\"))\n",
    "              symptom = symptom.replace('.','')\n",
    "              symptom = symptom.replace(';',',')\n",
    "              symptom=re.sub(r'<b.*?/b>:',',',symptom) # Remove bold text\n",
    "              symptom=re.sub(r'<a.*?>','',symptom) # Remove hyperlink\n",
    "              symptom=re.sub(r'</a>','',symptom) # Remove hyperlink\n",
    "              symptom=re.sub(r'<[^<]+?>',', ',symptom) # All the tags\n",
    "              symptom=re.sub(r'\\[.*\\]','',symptom) # Remove citation text\n",
    "              symptom=' '.join([x for x in symptom.split() if x != ','])\n",
    "              dis_symp[dis]=symptom\n",
    "              # print(dis_symp[dis])\n",
    "              filled = 1\n",
    "              break\n",
    "    if filled==1:\n",
    "      break\n",
    "      \n",
    "#for key,value in dis_symp.items():\n",
    "#  print(key,':',value)\n",
    "\n",
    "# Remove diseases that show duplicate symptoms list\n",
    "temp_list=[]\n",
    "tmp_dict=dict()\n",
    "for key,value in dis_symp.items():\n",
    "  if value not in temp_list:\n",
    "    tmp_dict[key]=value\n",
    "    temp_list.append(value)\n",
    "  else:\n",
    "    print(key)\n",
    "\n",
    "# Save the dictionary in PICKLE file\n",
    "dis_symp = tmp_dict\n",
    "print(len(dis_symp))\n",
    "with open('final_dis_symp.pickle', 'wb') as handle:\n",
    "   pickle.dump(dis_symp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4ab33",
   "metadata": {},
   "source": [
    "## Generating dataset\n",
    "Generating dataset from the scrapped data and pre processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7b83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
